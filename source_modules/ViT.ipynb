{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28*28 - 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "NUM_CLASSES = 10\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 28\n",
    "IN_CHANNELS = 1\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.001\n",
    "HIDDEN_DIM = 768 # hidden dimension of MLP head\n",
    "ADAM_WEIGHT_DECAY = 0\n",
    "ADAM_BETAS = (0.9, 0.999)\n",
    "ACTIVATION = \"gelu\"\n",
    "NUM_ENCODERS = 4\n",
    "EMBED_DIM = (PATCH_SIZE ** 2) * IN_CHANNELS # flattened patch dimension same as number ofelements in patch matrix\n",
    "NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim, patch_size, num_patches, dropout, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                # can get overlapping patches here by different stride values\n",
    "                stride=patch_size,\n",
    "            ),\n",
    "            # for batches dim 2\n",
    "            nn.Flatten(2)\n",
    "        )\n",
    "        \n",
    "        # to make learnable embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn((1, in_channels, embed_dim)), requires_grad=True)\n",
    "        \n",
    "        # learnable position embeddings\n",
    "        self.position_embeddings = nn.Parameter(torch.randn((1, num_patches+1, embed_dim)), requires_grad=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        \n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        print(x.shape)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 49, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "model = PatchEmbedding(EMBED_DIM, PATCH_SIZE, NUM_PATCHES, DROPOUT, IN_CHANNELS)\n",
    "model(torch.randn((10, 1, 28, 28))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_patches, img_size, num_classes, patch_size, embed_dim, num_encoders, num_heads, hidden_dim, dropout, activation, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n",
    "        \n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings_block(x)\n",
    "        print(\"Patch embeddings: \", x.shape)\n",
    "        \n",
    "        x = self.encoder_blocks(x)\n",
    "        print(\"Encode output: \", x.shape)\n",
    "        \n",
    "        # only first token used to make prediction of _class_\n",
    "        x = self.mlp_head(x[:, 0, :])\n",
    "        print(\"MLP Head output: \", x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ankush\\anaconda3\\envs\\his\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch embeddings:  torch.Size([512, 50, 16])\n",
      "Encode output:  torch.Size([512, 50, 16])\n",
      "MLP Head output:  torch.Size([512, 10])\n",
      "torch.Size([512, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ViT(NUM_PATCHES, IMG_SIZE, NUM_CLASSES, PATCH_SIZE, EMBED_DIM, NUM_ENCODERS, NUM_HEADS, HIDDEN_DIM, DROPOUT, ACTIVATION, IN_CHANNELS).to(device)\n",
    "x = torch.randn(512, 1, 28, 28).to(device)\n",
    "print(model(x).shape) # BATCH_SIZE X NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276_298\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sum(p.numel() for p in model.parameters()):_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding_raw(nn.Module):\n",
    "    \"\"\"\n",
    "    Breaks down raw image to linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, patch_size, num_patches):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # learnable position embeddings\n",
    "        self.position_embeddings = nn.Parameter(torch.randn((1, num_patches, in_channels*patch_size**2)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.shape\n",
    "        x = (x\n",
    "             .permute(0, 2, 3, 1)\n",
    "             .flatten(1)\n",
    "             .reshape(b, self.num_patches, self.in_channels*self.patch_size**2))\n",
    "        \n",
    "        x = x + self.position_embeddings\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 12])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PatchEmbedding_raw(3, 2, 4)\n",
    "model(torch.tensor(range(4*4*3*2))\n",
    " .reshape((2,3,4,4))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  9, 18],\n",
       "         [ 1, 10, 19],\n",
       "         [ 2, 11, 20],\n",
       "         [ 3, 12, 21],\n",
       "         [ 4, 13, 22],\n",
       "         [ 5, 14, 23],\n",
       "         [ 6, 15, 24],\n",
       "         [ 7, 16, 25],\n",
       "         [ 8, 17, 26]],\n",
       "\n",
       "        [[27, 36, 45],\n",
       "         [28, 37, 46],\n",
       "         [29, 38, 47],\n",
       "         [30, 39, 48],\n",
       "         [31, 40, 49],\n",
       "         [32, 41, 50],\n",
       "         [33, 42, 51],\n",
       "         [34, 43, 52],\n",
       "         [35, 44, 53]]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline to linearize pixels of image\n",
    "(torch.tensor(range(27*2))\n",
    " .reshape((2,3,3,3))\n",
    " .permute(0, 2, 3, 1)\n",
    " .flatten(1)\n",
    " .reshape(2, 3*3, 3) # batch, num tokens, items in each\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 300])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PatchEmbedding_raw(in_channels=3, \n",
    "                           patch_size=10,\n",
    "                           num_patches=(50//10)**2)\n",
    "\n",
    "model(torch.randn(1, 3, 50, 50)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrasnformerSegmenter(nn.Module):\n",
    "    def __init__(self, patcher, segmentation_head, embed_dim, num_encoders, num_heads, dropout, activation):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = patcher\n",
    "                \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, activation=activation, batch_first=True)\n",
    "        \n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
    "        \n",
    "        self.segmentation_head = segmentation_head\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        \n",
    "        x = self.patcher(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = self.encoder_blocks(x)\n",
    "        print(x.shape)\n",
    "        \n",
    "        # imageify\n",
    "        x = x.reshape(shape)\n",
    "        \n",
    "        x = self.segmentation_head(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrasnformerSegmenter(\n",
    "    PatchEmbedding_raw(3, 2, (50//2)**2),\n",
    "    nn.Sequential(\n",
    "        nn.Conv2d(3, 64, 3, 1, 1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 32, 3, 1, 1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 1, 3, 1, 1),\n",
    "    ),\n",
    "    3* (2*2),\n",
    "    2,\n",
    "    4,\n",
    "    0.1,\n",
    "    \"gelu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1,  2],\n",
       "          [ 3,  4,  5],\n",
       "          [ 6,  7,  8]],\n",
       "\n",
       "         [[ 9, 10, 11],\n",
       "          [12, 13, 14],\n",
       "          [15, 16, 17]],\n",
       "\n",
       "         [[18, 19, 20],\n",
       "          [21, 22, 23],\n",
       "          [24, 25, 26]]],\n",
       "\n",
       "\n",
       "        [[[27, 28, 29],\n",
       "          [30, 31, 32],\n",
       "          [33, 34, 35]],\n",
       "\n",
       "         [[36, 37, 38],\n",
       "          [39, 40, 41],\n",
       "          [42, 43, 44]],\n",
       "\n",
       "         [[45, 46, 47],\n",
       "          [48, 49, 50],\n",
       "          [51, 52, 53]]]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline to linearize pixels of image\n",
    "(torch.tensor(range(27*2))\n",
    " .reshape((2,3,3,3))\n",
    "#  .permute(0, 2, 3, 1)\n",
    "#  .flatten(1)\n",
    "#  .reshape(2, 3*3, 3) # batch, num tokens, items in each\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1,  2],\n",
       "          [ 3,  4,  5],\n",
       "          [ 6,  7,  8]],\n",
       "\n",
       "         [[ 9, 10, 11],\n",
       "          [12, 13, 14],\n",
       "          [15, 16, 17]],\n",
       "\n",
       "         [[18, 19, 20],\n",
       "          [21, 22, 23],\n",
       "          [24, 25, 26]]],\n",
       "\n",
       "\n",
       "        [[[27, 28, 29],\n",
       "          [30, 31, 32],\n",
       "          [33, 34, 35]],\n",
       "\n",
       "         [[36, 37, 38],\n",
       "          [39, 40, 41],\n",
       "          [42, 43, 44]],\n",
       "\n",
       "         [[45, 46, 47],\n",
       "          [48, 49, 50],\n",
       "          [51, 52, 53]]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline to imagify the encoder output to [2, 3, 3, 3]\n",
    "(torch.tensor(range(27*2))\n",
    " .reshape((2,9,3))\n",
    " .reshape((2, 3, 3, 3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 625, 12])\n",
      "torch.Size([2, 625, 12])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 50, 50])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn((2, 3, 50, 50))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, num_patches, img_size, num_classes, patch_size, embed_dim, num_encoders, num_heads, hidden_dim, dropout, activation, in_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings_block = PatchEmbedding(embed_dim, patch_size, num_patches, dropout, in_channels)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, activation=activation, batch_first=True, norm_first=True)\n",
    "        \n",
    "        self.encoder_blocks = nn.TransformerEncoder(encoder_layer, num_layers=num_encoders)\n",
    "        \n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embed_dim),\n",
    "            nn.Linear(in_features=embed_dim, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings_block(x)\n",
    "        print(\"Patch embeddings: \", x.shape)\n",
    "        \n",
    "        x = self.encoder_blocks(x)\n",
    "        print(\"Encode output: \", x.shape)\n",
    "        \n",
    "        # only first token used to make prediction of _class_\n",
    "        x = self.mlp_head(x[:, 0, :])\n",
    "        print(\"MLP Head output: \", x.shape)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "his",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
