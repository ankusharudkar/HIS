{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from his.rcnn_utils import *\n",
    "import his.metrics\n",
    "import his.models as crnn\n",
    "from torchvision.transforms import v2, transforms\n",
    "from torch.nn import init\n",
    "from collections import namedtuple\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from discordwebhook import Discord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets:\n",
    "\n",
    "- RP500, RP1000\n",
    "- RT500, RT1000\n",
    "- PP500, PP1000\n",
    "- PT500, PP1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split(loader: Dataset, split: list, seed: torch.Generator) -> Data:\n",
    "    return Data(*random_split(\n",
    "        loader,\n",
    "        split,\n",
    "        seed\n",
    "    )[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "Dataset = namedtuple(\"Dataset\", [\"name\", \"train\", \"val\", \"test\"])\n",
    "\n",
    "# datasets \n",
    "dRP500 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DRect_plain_1000/data/\", \"../data/DRect_plain_1000/RCNNAnnotations/\"),\n",
    "    [400, 50, 50, 500],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dRP500 = Dataset(\"dRP500\", dRP500.train, dRP500.val, dRP500.test)\n",
    "\n",
    "dRP1000 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DRect_plain_1000/data/\", \"../data/DRect_plain_1000/RCNNAnnotations/\"),\n",
    "    [800, 100, 100],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dRP1000 = Dataset(\"dRP1000\", dRP1000.train, dRP1000.val, dRP1000.test)\n",
    "\n",
    "dRT500 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DRect_texture_1000/data/\", \"../data/DRect_texture_1000/RCNNAnnotations/\"),\n",
    "    [400, 50, 50, 500],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dRT500 = Dataset(\"dRT500\", dRT500.train, dRT500.val, dRT500.test)\n",
    "\n",
    "dRT1000 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DRect_texture_1000/data/\", \"../data/DRect_texture_1000/RCNNAnnotations/\"),\n",
    "    [800, 100, 100],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dRT1000 = Dataset(\"dRT1000\", dRT1000.train, dRT1000.val, dRT1000.test)\n",
    "\n",
    "dPP500 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DPoly_plain_1000/data/\", \"../data/DPoly_plain_1000/RCNNAnnotations/\", transforms=v2.Resize((200,200), antialias=True)),\n",
    "    [400, 50, 50, 500],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dPP500 = Dataset(\"dPP500\", dPP500.train, dPP500.val, dPP500.test)\n",
    "\n",
    "dPP1000 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DPoly_plain_1000/data/\", \"../data/DPoly_plain_1000/RCNNAnnotations/\", transforms=v2.Resize((200,200), antialias=True)),\n",
    "    [800, 100, 100],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dPP1000 = Dataset(\"dPP1000\", dPP1000.train, dPP1000.val, dPP1000.test)\n",
    "\n",
    "dPT500 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DPoly_texture_1000/data/\", \"../data/DPoly_texture_1000/RCNNAnnotations/\", transforms=v2.Resize((200,200), antialias=True)),\n",
    "    [400, 50, 50, 500],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dPT500 = Dataset(\"dPT500\", dPT500.train, dPT500.val, dPT500.test)\n",
    "\n",
    "dPT1000 = load_and_split(\n",
    "    BaseRcnnDataset(\"../data/DPoly_texture_1000/data/\", \"../data/DPoly_texture_1000/RCNNAnnotations/\", transforms=v2.Resize((200,200), antialias=True)),\n",
    "    [800, 100, 100],\n",
    "    torch.Generator().manual_seed(10)\n",
    "    )\n",
    "dPT1000 = Dataset(\"dPT1000\", dPT1000.train, dPT1000.val, dPT1000.test)\n",
    "\n",
    "DATASETS = [\n",
    "    dRP500, dRP1000,\n",
    "    dRT500, dRT1000,\n",
    "    dPP500, dPP1000,\n",
    "    dPT500, dPT1000\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 200])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dPP500.val[0][1][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Base cells:\n",
    "- RNN-cell\n",
    "- LSTM-cell\n",
    "- GRU-cell\n",
    "\n",
    "Variants:\n",
    "- 2 Convolutional Layers\n",
    "- SegNet\n",
    "- UNet\n",
    "\n",
    "$3\\times 3 = 9$ combinations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base cells\n",
    "\n",
    "- 2 Convolutiona layers\n",
    "- SegNet\n",
    "- UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2Layers(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.process = nn.Sequential(\n",
    "          nn.Conv2d(in_channels, in_channels, 5, 1, 2),\n",
    "          nn.BatchNorm2d(in_channels),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "        )\n",
    "        \n",
    "        init.orthogonal_(self.process[0].weight)\n",
    "        init.orthogonal_(self.process[-1].weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.process(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegNet = lambda x, y: nn.Sequential(\n",
    "    nn.Conv2d(x, 64, 3, 1, 1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    \n",
    "    nn.Conv2d(64, 128, 3, 1, 1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    \n",
    "    nn.Conv2d(128, 128, 3, 1, 1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.05),\n",
    "    \n",
    "    nn.ConvTranspose2d(128, 64, 2, 2),\n",
    "    nn.Conv2d(64, 64, 3, 1, 1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.ConvTranspose2d(64, 32, 2, 2),\n",
    "    nn.Conv2d(32, 32, 3, 1, 1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, y, 3, 1, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False), # no bias for batch norm\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3, \n",
    "        out_channels=1,\n",
    "        features=[64, 128, 256, 512],\n",
    "        dropout=0.,\n",
    "        DoubleConv = DoubleConv\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # downs\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "            \n",
    "        # ups\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2,\n",
    "                    feature,\n",
    "                    2,\n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "            \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "            x = nn.functional.dropout2d(x, self.dropout)\n",
    "            \n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            \n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:], antialias=True)\n",
    "            \n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "            x = nn.functional.dropout2d(x, self.dropout)\n",
    "            \n",
    "        return self.final_conv(x)\n",
    "    \n",
    "    def grad_norm(self):\n",
    "        total_norm = 0.0\n",
    "        for param in self.parameters():\n",
    "            if param.grad is not None:\n",
    "                param_norm = param.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "\n",
    "        return total_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = namedtuple('Model', ['name', 'init'])\n",
    "\n",
    "MODELS = [\n",
    "    Model('RNN-2C', lambda: crnn.ConvRNNCell(3, 200, 200, Conv2Layers)),\n",
    "    Model('RNN-SegNet', lambda: crnn.ConvRNNCell(3, 200, 200, SegNet)),\n",
    "    Model('RNN-UNet', lambda: crnn.ConvRNNCell(3, 200, 200, UNet)),\n",
    "    \n",
    "    Model('LSTM-2C', lambda: crnn.ConvLSTMCell(3, 200, 200, Conv2Layers)),\n",
    "    Model('LSTM-SegNet', lambda: crnn.ConvLSTMCell(3, 200, 200, SegNet)),\n",
    "    Model('LSTM-UNet', lambda: crnn.ConvLSTMCell(3, 200, 200, UNet)),\n",
    "    \n",
    "    # Model('GRU-2C', lambda: crnn.ConvGRUCell(3, 200, 200, Conv2Layers)),\n",
    "    # Model('GRU-SegNet', lambda: crnn.ConvGRUCell(3, 200, 200, SegNet)),\n",
    "    # Model('GRU-UNet', lambda: crnn.ConvGRUCell(3, 200, 200, UNet)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(prediction, target, threshold=0.5, smooth=1e-5, norm=1):\n",
    "    prediction_binary = (prediction > threshold).float()\n",
    "    \n",
    "    target /= norm\n",
    "    intersection = torch.sum(prediction_binary * target)\n",
    "    union = prediction_binary + target\n",
    "    union[union > 1] = 1\n",
    "    union = torch.sum(union)\n",
    "\n",
    "    dice = (intersection + smooth) / (union + smooth)\n",
    "    return dice.cpu().detach()\n",
    "\n",
    "def iou_score(model, dataset, threshold=0.5, device=\"cuda\", norm=1):\n",
    "    score_sum = 0\n",
    "    count = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(dataset):\n",
    "            input, target = data\n",
    "            input, target = input.to(device), [torch.tensor(t).to(device) for t in target]\n",
    "            model.resetState()\n",
    "                \n",
    "            prediction = model(input.unsqueeze(dim=0))\n",
    "            \n",
    "            for t in target:\n",
    "                score_sum += float(iou(prediction.squeeze(), t/norm, threshold))\n",
    "                prediction = model(input.unsqueeze(dim=0))\n",
    "                count += 1\n",
    "                \n",
    "            print(f\"{i:4d}/{len(dataset)}\", end=\"\\r\")\n",
    "\n",
    "    return score_sum/count\n",
    "\n",
    "def binary_dice_coefficient(prediction, target, threshold=0.5, smooth=1e-5):\n",
    "    prediction_binary = (prediction > threshold).float()\n",
    "    \n",
    "    intersection = torch.sum(prediction_binary * target)\n",
    "    union = torch.sum(prediction_binary) + torch.sum(target)\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return dice.cpu().detach()\n",
    "\n",
    "\n",
    "def dice_score(model, dataset, threshold=0.5, device=\"cuda\", norm=1):\n",
    "    score_sum = 0\n",
    "    count = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(dataset):\n",
    "            input, target = data\n",
    "            input, target = input.to(device), [torch.tensor(t).to(device) for t in target]\n",
    "            \n",
    "            model.resetState()\n",
    "            \n",
    "            prediction = model(input.unsqueeze(dim=0))\n",
    "            \n",
    "            for t in target:\n",
    "                score_sum += float(binary_dice_coefficient(prediction.squeeze(), t/norm, threshold))\n",
    "                prediction = model(input.unsqueeze(dim=0))\n",
    "                count += 1\n",
    "            \n",
    "            print(f\"{i:4d}/{len(dataset)}\", end=\"\\r\")\n",
    "\n",
    "    return score_sum/count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset: Data, \n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    loss: torch.nn.Module, \n",
    "    writer: SummaryWriter,\n",
    "    n_epochs: tuple=(0, 50),\n",
    "    device: str=\"cuda\",\n",
    "    dice: bool=False,\n",
    "    norm: float=1.0,\n",
    "    log: Discord=None):\n",
    "    \n",
    "    for epoch in range(*n_epochs):\n",
    "        epoch_loss = 0\n",
    "        for data in dataset.train:\n",
    "            # reset hidden state\n",
    "            model.resetState()\n",
    "            \n",
    "            image, annotation = data\n",
    "            image = image.to(device).unsqueeze(dim=0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(image)\n",
    "            \n",
    "            l = 0\n",
    "        \n",
    "            for weight, ann in enumerate(annotation):\n",
    "                ann = torch.tensor(ann).to(\"cuda\").unsqueeze(dim=0).unsqueeze(dim=0).float()/norm\n",
    "                l += loss(output, ann)\n",
    "                # next step input                \n",
    "                output = model(image)\n",
    "            \n",
    "            l.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_loss += float(l)\n",
    "            \n",
    "        epoch_loss /= len(dataset.train)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        print(f\"Epoch {epoch}: {epoch_loss}\")\n",
    "            \n",
    "        # val\n",
    "        with torch.inference_mode():\n",
    "            val_loss = 0\n",
    "            for data in dataset.val:\n",
    "                # reset hidden state\n",
    "                model.resetState()\n",
    "                \n",
    "                image, annotation = data\n",
    "                image = image.to(device).unsqueeze(dim=0)\n",
    "                \n",
    "                output = model(image)\n",
    "                \n",
    "                l = 0\n",
    "            \n",
    "                for weight, ann in enumerate(annotation):\n",
    "                    ann = torch.tensor(ann).to(\"cuda\").unsqueeze(dim=0).unsqueeze(dim=0).float()/norm\n",
    "                    l += loss(output, ann)\n",
    "                    # next step input                \n",
    "                    output = model(image)\n",
    "                    \n",
    "                val_loss += float(l)\n",
    "                \n",
    "            val_loss /= len(dataset.val)\n",
    "            writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "            writer.add_scalar(\"GradNorm\", model.grad_norm(), epoch)\n",
    "            \n",
    "            if log and epoch%50==0:\n",
    "                log.post(content=f\"Train loss: {float(epoch_loss)}, Val loss:{float(val_loss)}\")\n",
    "            \n",
    "    if not dice:\n",
    "        return\n",
    "    \n",
    "    dices = []\n",
    "    ious = []\n",
    "    for i, thresh in enumerate(np.arange(0, 1.1, 0.1)):\n",
    "        train_dice = dice_score(model, dataset.train, thresh)\n",
    "        val_dice = dice_score(model, dataset.val, thresh)\n",
    "        writer.add_scalar(\"Dice/train\", train_dice, i)\n",
    "        writer.add_scalar(\"Dice/val\", val_dice, i)\n",
    "        dices.append(train_dice)\n",
    "        \n",
    "        train_iou = iou_score(model, dataset.train, thresh)\n",
    "        val_iou = iou_score(model, dataset.val, thresh)\n",
    "        writer.add_scalar(\"IoU/train\", train_iou, i)\n",
    "        writer.add_scalar(\"IoU/val\", val_iou, i)\n",
    "        ious.append(train_iou)\n",
    "        \n",
    "    return dices, ious\n",
    "            \n",
    "def n_params(model):\n",
    "    return sum(param.numel() for param in model.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(model, dataset, idx, in_channels=3, height=200, width=200):\n",
    "    with torch.inference_mode():\n",
    "        image, annotation = dataset[idx]\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        plt.subplot(1, 7, 1)\n",
    "        plt.imshow(image.permute(1,2,0))\n",
    "        plt.axis(\"off\")\n",
    "        \n",
    "        image = (image).unsqueeze(dim=0).to(\"cuda\")\n",
    "        \n",
    "        model.hidden = torch.ones(1, in_channels, height, width).to(\"cuda\")\n",
    "        for i in range(5):\n",
    "            plt.subplot(1, 7, i+2)\n",
    "            output = model(image)\n",
    "            \n",
    "            plt.imshow(output.detach().cpu().squeeze())\n",
    "            # plt.imshow(annotation[i])\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "        plt.savefig(\"fig.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = Discord(url=\"https://discord.com/api/webhooks/1219933822351577088/Gz4O9UZlnG7QBGTb9v5wPKlzxO3N8y_r9efsjFMOuHRki_Ok0Xw7e47P2eLt0A7DeA11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchy_score(model, dataset, threshold=0.5, device=\"cuda\", norm=1):\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1 = 0\n",
    "    count = 0\n",
    "    with torch.inference_mode():\n",
    "        for i, data in enumerate(dataset):\n",
    "            input, target = data\n",
    "            input, target = input.to(device), [torch.tensor(t).to(device) for t in target]\n",
    "            \n",
    "            model.resetState()\n",
    "            \n",
    "            prediction = model(input.unsqueeze(dim=0))\n",
    "            t_in = []\n",
    "            pred_in = []\n",
    "            for t in target[:-1]:\n",
    "                t_in.append(t.cpu().numpy().astype(int))\n",
    "                pred_in.append((prediction.squeeze().cpu().numpy() >= threshold).astype(int))\n",
    "                prediction = model(input.unsqueeze(dim=0))\n",
    "                count += 1\n",
    "                \n",
    "            his_metrics = his.metrics.hierarchical_metrics(t_in, pred_in)\n",
    "            precision += his_metrics[\"hierarchical-precision\"]\n",
    "            recall += his_metrics[\"hierarchical-recall\"]\n",
    "            f1 += his_metrics[\"hierarchical-f1\"]\n",
    "            \n",
    "            print(f\"{i:4d}/{len(dataset)}\", end=\"\\r\")\n",
    "\n",
    "    return precision/count, recall/count, f1/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_datetime():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 3.591641210317612\n",
      "(0.0005106491484930975, 0.007271064822452691, 0.000870644167510116)\n",
      "(0.0005900433904989644, 0.00752542970499739, 0.0010198832510423686)\n",
      "fin RNN-2C, dRP500\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "# get combination of datasets and models\n",
    "try:\n",
    "    for i, (model_def, dataset) in enumerate(product(MODELS, DATASETS)):\n",
    "        # initialize model trainers\n",
    "        # -------log-----------\n",
    "        log.post(content=f\"Training model {model_def.name}, {dataset.name} {get_datetime()}⭐\")\n",
    "        # -------log-----------\n",
    "        \n",
    "        model = model_def.init().cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "        loss = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(4.).cuda())\n",
    "        writer = SummaryWriter(f\"./runs/{model_def.name}_{dataset.name}\", comment=\"BCE_pos=4.0\")\n",
    "        \n",
    "        # training\n",
    "        dices, ious = train(dataset,\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss,\n",
    "        writer,\n",
    "        n_epochs=(0, n_epochs),\n",
    "        dice=True,\n",
    "        log=log)\n",
    "        \n",
    "        size = dataset.val[0][1][0].shape[0]\n",
    "        show_results(model, dataset.val, 10, height=size, width=size)\n",
    "        # -------log-----------\n",
    "        with open(\"fig.png\", \"rb\") as f:\n",
    "            log.post(content=f\"Result of {model_def.name} on {dataset.name}\", file={\n",
    "                \"output.png\": f\n",
    "            })\n",
    "        # -------log-----------\n",
    "        \n",
    "        # computing hierarchy metrics for scores\n",
    "        threshold = np.argmax(ious)/10\n",
    "        h_train = hierarchy_score(model, dataset.train, threshold)\n",
    "        h_val = hierarchy_score(model, dataset.val, threshold)\n",
    "        print(h_train)\n",
    "        print(h_val)\n",
    "        writer.add_scalar(\"HIS/Precision/train\", h_train[0])\n",
    "        writer.add_scalar(\"HIS/Recall/train\", h_train[1])\n",
    "        writer.add_scalar(\"HIS/F1/train\", h_train[2])\n",
    "        writer.add_scalar(\"HIS/Precision/val\", h_val[0])\n",
    "        writer.add_scalar(\"HIS/Recall/val\", h_val[1])\n",
    "        writer.add_scalar(\"HIS/F1/train\", h_val[2])\n",
    "        \n",
    "        # -------log-----------\n",
    "        log.post(content=f\"Training:\\n Precision: {h_train[0]}, Recall: {h_train[1]}, F1: {h_train[2]}\")\n",
    "        log.post(content=f\"Validation:\\n Precision: {h_val[0]}, Recall: {h_val[1]}, F1: {h_val[2]}\")\n",
    "        log.post(content=f\"Training finished {get_datetime()} ✅\")\n",
    "        # -------log-----------\n",
    "        # print(f\"fin {model_def.name}, {dataset.name}\")\n",
    "except Exception as e:\n",
    "    log.post(content=f\"Training failed ❌: with {e}\")\n",
    "    \n",
    "log.post(content=f\"All Training finished {get_datetime()} ✅✨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(i1==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_mask_tree(ops).traverse_inorder(lambda x, y: print(\" \"*y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(i1==10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # getting some layer of outputs\n",
    "# import pickle\n",
    "# model.resetState()\n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     image, target = dRP500.train[0]\n",
    "#     output1 = model(image.unsqueeze(dim=0).cuda())\n",
    "#     output2 = model(image.unsqueeze(dim=0).cuda())\n",
    "#     output3 = model(image.unsqueeze(dim=0).cuda())\n",
    "    \n",
    "# plt.imshow(output1.cpu().squeeze() > 0.1)\n",
    "# with open(\"dummy_op.pkl\", \"wb\") as f:\n",
    "#     pickle.dump([o.cpu().squeeze().numpy() > 0.1 for o in [output1, output2, output3]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"dummy_op.pkl\", \"rb\") as r:\n",
    "#     ops = pickle.load(r)\n",
    "#     ops = [o.astype(int) for o in ops]\n",
    "    \n",
    "# i1 = ops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(i1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i1 = ops[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_train = hierarchy_score(model, dataset.train, threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "his",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
